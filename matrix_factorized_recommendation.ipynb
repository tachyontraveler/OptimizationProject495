{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "http://kitchingroup.cheme.cmu.edu/blog/2017/11/18/Neural-networks-for-regression-with-autograd/\n",
    "\n",
    "https://github.com/HIPS/autograd/blob/master/autograd/misc/optimizers.py\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-recommendation-engine-python/\n",
    "\n",
    "https://medium.com/datadriveninvestor/how-to-built-a-recommender-system-rs-616c988d64b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd import numpy as np\n",
    "from autograd import grad\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datetime import datetime\n",
    "from multiprocessing import Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py\n",
    "import sys\n",
    "import autograd.numpy.random as npr\n",
    "import pickle\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Defaults and Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = './datafiles/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OWrite(s):\n",
    "    print(s)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "def saveh5py(hdata,hname):\n",
    "    with h5py.File(datadir+hname, 'w') as h5f:\n",
    "        h5f.create_dataset('dataset', data=hdata)\n",
    "\n",
    "def readh5py(hname):\n",
    "    with h5py.File(datadir+hname,'r') as h5f:\n",
    "        hdata = h5f['dataset'][:]\n",
    "    return hdata\n",
    "\n",
    "def gen_dmatrix(data,matrix_shape):\n",
    "    initTime = datetime.now()\n",
    "    dmatrix = np.zeros(matrix_shape)\n",
    "    for item in data:\n",
    "        dmatrix[item[0],item[1]] = item[2]\n",
    "    saveh5py(dmatrix,'dmatrix.h5')\n",
    "    OWrite (\"Time spent on computing data matrix: \"+str(datetime.now() - initTime))\n",
    "    return dmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecSys():\n",
    "    def __init__(self,args):\n",
    "        \n",
    "        self.L           = args['L']\n",
    "        self.alpha       = args['alpha']\n",
    "        self.xtest       = args['xtest']\n",
    "        self.xtrain      = args['xtrain']\n",
    "        self.dmatrix     = args['dmatrix']\n",
    "        self.num_users   = len(args['unique_cust_ids_list'])\n",
    "        self.num_movies  = len(args['unique_movie_ids_list'])\n",
    "        self.max_epochs  = args['max_epochs']\n",
    "        self.gradient_fn = grad(self.squared_error,0)\n",
    "        \n",
    "        self.UL = np.random.normal(scale=1./self.L, size=(self.num_users, self.L))\n",
    "        self.ML = np.random.normal(scale=1./self.L, size=(self.num_movies, self.L))\n",
    "        self.BU = np.zeros((self.num_users,1))\n",
    "        self.BM = np.zeros((self.num_movies,1))\n",
    "        self.b  = np.mean(xtrain[:,-1])\n",
    "        \n",
    "\n",
    "    def model(self,params):\n",
    "        ul,ml,bu,bm = params\n",
    "        return self.b + bu + bm + np.dot(ul,ml.T)\n",
    "\n",
    "    def squared_error(self,params,y):\n",
    "        return (np.square(y-self.model(params)))\n",
    "    \n",
    "    def gradient_descent(self):\n",
    "        for xtem in self.xtrain:\n",
    "            params = (self.UL[xtem[0]],self.ML[xtem[1]],self.BU[xtem[0]],self.BM[xtem[1]])\n",
    "            step_val = tuple(self.alpha*stem for stem in self.gradient_fn(params,xtem[2]))\n",
    "            self.UL[xtem[0]] = params[0] - step_val[0]\n",
    "            self.ML[xtem[1]] = params[1] - step_val[1]\n",
    "            self.BU[xtem[0]] = params[2] - step_val[2]\n",
    "            self.BM[xtem[1]] = params[3] - step_val[3]\n",
    "        \n",
    "    def train(self):\n",
    "        lossdata = []\n",
    "        for i_e in range(self.max_epochs):\n",
    "            initTime = datetime.now()\n",
    "            np.random.shuffle(self.xtrain)\n",
    "            self.gradient_descent()\n",
    "            avg_errors = self.train_mse()\n",
    "            lossdata.append(avg_errors)\n",
    "            OWrite(\"Epoch: {} \\t MSE(train): {:.4f} \\t MSE(test): {:.4f} \\t TimeConsumed: {}\".format(\n",
    "                                i_e+1,avg_errors[0],avg_errors[1],(datetime.now() - initTime).total_seconds()))\n",
    "        return lossdata\n",
    "            \n",
    "            \n",
    "    def predict_matrix(self):\n",
    "        return self.b + (recc.BU[:,np.newaxis]+recc.BM[np.newaxis:,]).squeeze() + np.dot(self.UL,self.ML.T)\n",
    "    \n",
    "    def train_mse(self):\n",
    "        xs, ys = self.dmatrix.nonzero()\n",
    "        pred_matrix = self.predict_matrix()\n",
    "        return (np.sqrt(sum([pow(self.dmatrix[x,y]-pred_matrix[x,y],2) for x,y in zip(xs,ys)])),\n",
    "               self.test_mse(pred_matrix))\n",
    "    \n",
    "    def test_mse(self,pred_matrix):\n",
    "        return np.sqrt(sum([pow(ttem[2]-pred_matrix[ttem[0],ttem[1]],2) for ttem in self.xtest]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_processed_data(test_size=0.1):\n",
    "    data           = readh5py('converted_final_data.h5')\n",
    "    list_cust_ids  = np.genfromtxt(datadir+'final_custids.csv',dtype=int)\n",
    "    list_movie_ids = np.genfromtxt(datadir+'final_movieids.csv',dtype=int)\n",
    "    OWrite (\"Splitting data to train and test sets\")\n",
    "    xtrain, xtest  = train_test_split(data, test_size=test_size, random_state=7)\n",
    "    dmatrix        = gen_dmatrix(xtrain,(len(list_cust_ids),len(list_movie_ids)))\n",
    "    \n",
    "    saveh5py(xtrain, 'traindata.h5')\n",
    "    saveh5py(xtest,  'testdata.h5')\n",
    "    saveh5py(dmatrix,'dmatrix.h5')\n",
    "    \n",
    "    OWrite (\"Shape of training data: \"+str(xtrain.shape))\n",
    "    OWrite (\"Shape of test data: \"+str(xtest.shape))\n",
    "    \n",
    "    return (xtrain,xtest,dmatrix,list_cust_ids,list_movie_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data to train and test sets\n",
      "Time spent on computing data matrix: 0:00:03.041529\n",
      "Shape of training data: (1251541, 3)\n",
      "Shape of test data: (139061, 3)\n"
     ]
    }
   ],
   "source": [
    "xtrain,xtest,dmatrix,list_cust_ids,list_movie_ids = read_processed_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {'alpha'     : 0.001,\n",
    "        'L'         : 10,\n",
    "        'xtest'     : xtest,\n",
    "        'xtrain'    : xtrain,\n",
    "        'dmatrix'   : dmatrix,\n",
    "        'max_epochs': 20,\n",
    "        'unique_cust_ids_list' : np.copy(list_cust_ids),\n",
    "        'unique_movie_ids_list': np.copy(list_movie_ids) ,\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "recc = RecSys(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t MSE(train): 1119.5992 \t MSE(test): 375.8544 \t TimeConsumed: 519.279897\n",
      "Epoch: 2 \t MSE(train): 1099.3258 \t MSE(test): 369.9343 \t TimeConsumed: 524.770667\n",
      "Epoch: 3 \t MSE(train): 1085.1510 \t MSE(test): 365.9858 \t TimeConsumed: 543.513432\n",
      "Epoch: 4 \t MSE(train): 1073.8230 \t MSE(test): 362.9543 \t TimeConsumed: 521.712245\n",
      "Epoch: 5 \t MSE(train): 1064.3526 \t MSE(test): 360.4954 \t TimeConsumed: 485.006403\n",
      "Epoch: 6 \t MSE(train): 1056.1794 \t MSE(test): 358.4168 \t TimeConsumed: 462.591984\n",
      "Epoch: 7 \t MSE(train): 1048.9547 \t MSE(test): 356.6984 \t TimeConsumed: 502.360174\n",
      "Epoch: 8 \t MSE(train): 1042.5757 \t MSE(test): 355.1282 \t TimeConsumed: 454.885383\n",
      "Epoch: 9 \t MSE(train): 1036.8401 \t MSE(test): 353.8504 \t TimeConsumed: 459.843534\n",
      "Epoch: 10 \t MSE(train): 1031.6463 \t MSE(test): 352.6771 \t TimeConsumed: 493.148391\n",
      "Epoch: 11 \t MSE(train): 1026.9208 \t MSE(test): 351.7079 \t TimeConsumed: 508.9621\n",
      "Epoch: 12 \t MSE(train): 1022.6137 \t MSE(test): 350.8735 \t TimeConsumed: 580.532152\n",
      "Epoch: 13 \t MSE(train): 1018.5668 \t MSE(test): 350.1548 \t TimeConsumed: 558.060742\n",
      "Epoch: 14 \t MSE(train): 1014.7455 \t MSE(test): 349.4892 \t TimeConsumed: 483.947105\n",
      "Epoch: 15 \t MSE(train): 1011.2029 \t MSE(test): 348.9572 \t TimeConsumed: 456.032241\n",
      "Epoch: 16 \t MSE(train): 1007.7910 \t MSE(test): 348.4869 \t TimeConsumed: 446.792705\n",
      "Epoch: 17 \t MSE(train): 1004.4904 \t MSE(test): 348.0900 \t TimeConsumed: 449.225645\n",
      "Epoch: 18 \t MSE(train): 1001.2540 \t MSE(test): 347.7513 \t TimeConsumed: 470.187451\n",
      "Epoch: 19 \t MSE(train): 998.0216 \t MSE(test): 347.4871 \t TimeConsumed: 454.885879\n",
      "Epoch: 20 \t MSE(train): 994.8308 \t MSE(test): 347.3153 \t TimeConsumed: 493.868793\n"
     ]
    }
   ],
   "source": [
    "# recc.alpha = \n",
    "# recc.max_epochs = \n",
    "\n",
    "lossvals = recc.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceeding to save the matrices as h5\n"
     ]
    }
   ],
   "source": [
    "save_params = True\n",
    "if save_params:\n",
    "    OWrite('Proceeding to save the matrices as h5')\n",
    "    train_vars = [recc.UL,recc.ML,recc.BU,recc.BM,recc.b]\n",
    "    \n",
    "    with h5py.File(datadir+'latest_model_trained_vars.h5', 'w') as h5f:\n",
    "        for i,item in enumerate(['UL','ML','BU','BM','b']):\n",
    "            h5f.create_dataset('dataset'+item, data=train_vars[i])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recc.alpha = \n",
    "#recc.max_epochs = \n",
    "#lossvals = recc.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (495_ann_py3.7)",
   "language": "python",
   "name": "eecs495_ann"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
