{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datetime import datetime\n",
    "from multiprocessing import Pool\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Defaults and Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_max_samples = 1000000\n",
    "n_custs   = 30000\n",
    "datadir = './datafiles/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveh5py(hdata,hname):\n",
    "    h5f = h5py.File(datadir+hname, 'w')\n",
    "    h5f.create_dataset('dataset', data=hdata)\n",
    "    h5f.close()\n",
    "\n",
    "def readh5py(hname):\n",
    "    h5f = h5py.File(datadir+hname,'r')\n",
    "    hdata = h5f['dataset'][:]\n",
    "    h5f.close()\n",
    "    return hdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data from raw Netflix Prize file(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(load_saved=False):\n",
    "    initTime = datetime.now()\n",
    "    if load_saved:\n",
    "        data = (np.genfromtxt(datadir+'current_full_data.csv',dtype=int))\n",
    "        print (\"Time spent in data load: \"+str(datetime.now() - initTime))\n",
    "        return data\n",
    "    data = []\n",
    "    for filename in ['1']: #['1','2','3','4']:\n",
    "        filename = '../dataset/combined_data_'+filename+'.txt'\n",
    "        for line in open(filename,'r').read().strip().split('\\n'):\n",
    "            line = line.strip()\n",
    "            if line.endswith(':'):\n",
    "                movie_title = (line[:-1])\n",
    "            else:\n",
    "                data.append([movie_title]+line.split(','))            \n",
    "    data = (np.array(data)[:,:-1]).astype(int)\n",
    "    title = 'movie,username,rating,timestamp'\n",
    "    np.savetxt(datadir+'current_full_data.csv',data,fmt=\"%i\",header=title)\n",
    "    print (\"Time spent in data load: \"+str(datetime.now() - initTime))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data sampling and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Sampling \n",
    "def sample_data(data):\n",
    "    return data[np.random.choice(data.shape[0], n_max_samples, replace=False),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get indices of randomly selected movies and customers and if needed, restrict their numbers\n",
    "def get_indices(data):\n",
    "    cust_ids = np.unique(data[:,1])\n",
    "    cust_ids = cust_ids[np.random.choice(len(cust_ids), n_custs, replace=False)]\n",
    "    cust_ids.sort()\n",
    "    cust_ids = list(cust_ids)\n",
    "    print('final number of customers: '+str(len(cust_ids)))\n",
    "    \n",
    "    movie_ids = np.unique(data[:,0])\n",
    "    movie_ids.sort()\n",
    "    movie_ids = list(movie_ids)\n",
    "    print('final number of movies: '+str(len(movie_ids)))\n",
    "    return cust_ids,movie_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load and process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling function to load data\n",
      "Time spent in data load: 0:02:07.687227\n",
      "Loaded full data\n"
     ]
    }
   ],
   "source": [
    "print (\"Calling function to load data\")\n",
    "tdata = read_data(load_saved=True)\n",
    "print (\"Loaded full data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled the data to include 1000000 datapoints\n",
      "final number of customers: 30000\n",
      "final number of movies: 4483\n",
      "Listed the IDs of coustomers and movies as global variables\n",
      "Saved the global variables to disk\n"
     ]
    }
   ],
   "source": [
    "xtrain = sample_data(tdata)\n",
    "print (\"Sampled the data to include {} datapoints\".format(n_max_samples))\n",
    "global_var_cust_ids, global_var_movie_ids = get_indices(xtrain)\n",
    "print (\"Listed the IDs of coustomers and movies as global variables\")\n",
    "np.savetxt(datadir+'final_custids.csv',global_var_cust_ids,fmt=\"%i\")\n",
    "np.savetxt(datadir+'final_movieids.csv',global_var_movie_ids,fmt=\"%i\")\n",
    "print (\"Saved the global variables to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Movie and User Indices to continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_indices(item):\n",
    "    if item[1] in global_var_cust_ids:\n",
    "        item[1] = global_var_cust_ids.index(item[1])+1\n",
    "        item[0] = global_var_movie_ids.index(item[0])+1\n",
    "    else:\n",
    "        item = [0,0,0]\n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User IDs are discontinuous in this dataset. Converting indices to continuous.\n",
      "Time spent on conversion: 0:04:58.442466\n",
      "Shape of final full data after conversion: (105595, 3)\n"
     ]
    }
   ],
   "source": [
    "print (\"User IDs are discontinuous in this dataset. Converting indices to continuous.\")\n",
    "initTime = datetime.now()\n",
    "p = Pool()\n",
    "xtrain = p.map(convert_indices,xtrain)\n",
    "print (\"Time spent on conversion: \"+str(datetime.now() - initTime))\n",
    "xtrain = np.array(xtrain)\n",
    "xtrain = xtrain[~np.all(xtrain == 0, axis=1)]\n",
    "print (\"Shape of final full data after conversion: \"+str(xtrain.shape))\n",
    "title = 'movie,username,rating'\n",
    "np.savetxt(datadir+'converted_final_data.csv',xtrain,fmt=\"%i\",header=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data to train and test sets\n",
      "Shape of training data: (84476, 3)\n",
      "Shape of test data: (21119, 3)\n"
     ]
    }
   ],
   "source": [
    "print (\"Splitting data to train and test sets\")\n",
    "xtrain, xtest = train_test_split(xtrain, test_size=0.2, random_state=7)\n",
    "saveh5py(xtrain,'traindata.h5')\n",
    "saveh5py(xtest,'testdata.h5')\n",
    "print (\"Shape of training data: \"+str(xtrain.shape))\n",
    "print (\"Shape of test data: \"+str(xtest.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute User-Item Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dmatrix(data):\n",
    "    dmatrix = np.zeros((len(global_var_cust_ids),len(global_var_movie_ids)))\n",
    "    for item in data:\n",
    "        dmatrix[item[1]-1][item[0]-1] = item[2]\n",
    "    saveh5py(dmatrix,'dmatrix.h5')\n",
    "    return dmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent on computing data matrix: 0:00:01.490281\n",
      "Shape of user-item matrix: (30000, 4483)\n",
      "Number of users: 30000 , Number of Items: 4483\n"
     ]
    }
   ],
   "source": [
    "initTime = datetime.now()\n",
    "dmatrix = gen_dmatrix(xtrain)\n",
    "print (\"Time spent on computing data matrix: \"+str(datetime.now() - initTime))\n",
    "print (\"Shape of user-item matrix: \"+str(dmatrix.shape))\n",
    "print (\"Number of users: {} , Number of Items: {}\".format(len(global_var_cust_ids),len(global_var_movie_ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute User-Item and Similarity Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initTime = datetime.now()\n",
    "#user_similarity = pairwise_distances(dmatrix, metric='cosine')\n",
    "#print (\"Time spent on user similarity matrix: \"+str(datetime.now() - initTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_user_similarity = True\n",
    "comp_movie_similarity = True\n",
    "sim_metric = 'cosine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 30000)\n",
      "Time spent on user similarity matrix: 0:02:36.823019\n"
     ]
    }
   ],
   "source": [
    "if comp_user_similarity:\n",
    "    initTime = datetime.now()\n",
    "    user_similarity = pairwise_distances(dmatrix, metric=sim_metric)\n",
    "    print (user_similarity.shape)\n",
    "    print (\"Time spent on user similarity matrix: \"+str(datetime.now() - initTime))\n",
    "    saveh5py(user_similarity,'usersim.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent on item similarity matrix: 0:00:18.076184\n",
      "(4483, 4483)\n"
     ]
    }
   ],
   "source": [
    "if comp_movie_similarity:\n",
    "    initTime = datetime.now()\n",
    "    item_similarity = pairwise_distances(dmatrix.T, metric=sim_metric)\n",
    "    print (\"Time spent on item similarity matrix: \"+str(datetime.now() - initTime))\n",
    "    print (item_similarity.shape)\n",
    "    saveh5py(item_similarity,'itemsim.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_predict(ratings, similarity):\n",
    "    print (\"Beginning User Predict\")\n",
    "    initTime = datetime.now()\n",
    "    mean_user_rating = ratings.mean(axis=1)\n",
    "    #We use np.newaxis so that mean_user_rating has same format as ratings\n",
    "    ratings_diff = (ratings - mean_user_rating[:, np.newaxis])\n",
    "    print (\"Proceeding to prediction\")\n",
    "    pred = mean_user_rating[:, np.newaxis] + similarity.dot(ratings_diff) / np.array([np.abs(similarity).sum(axis=1)]).T\n",
    "    print (\"Time spent on computing prediction matrix: \"+str(datetime.now() - initTime))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movie_predict(ratings, similarity):\n",
    "    pred = ratings.dot(similarity) / np.array([np.abs(similarity).sum(axis=1)])\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning User Predict\n",
      "Proceeding to prediction\n",
      "Time spent on computing prediction matrix: 0:06:17.005595\n"
     ]
    }
   ],
   "source": [
    "predictions_user = user_predict(dmatrix,user_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_movie = movie_predict(dmatrix,item_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveh5py(predictions_user,'userpreds.h5')\n",
    "saveh5py(predictions_movie,'itempreds.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (495_ann_py3.7)",
   "language": "python",
   "name": "eecs495_ann"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
